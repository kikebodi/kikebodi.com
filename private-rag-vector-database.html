<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  <title>Building a Private RAG System with Ollama, LangChain and Chroma - LLM Engineer | AI Agent Engineer | Senior Software Developer</title>

    <!-- Open Graph -->
<meta property="og:type" content="article" />
<meta property="og:title" content="Building a Private RAG System with LangChain, Chroma, and Local LLMs" />
<meta property="og:description" content="End-to-end guide to building a private Retrieval-Augmented Generation (RAG) system using LangChain, Chroma, and local/open-source LLMs. Includes embeddings, chunking, vector search, and evaluation." />
<meta property="og:url" content="https://kikebodi.com/private-rag-vector-database.html" />
<meta property="og:image" content="https://kikebodi.com/img/rag_diagram.png" />
<meta property="og:site_name" content="Kike Bodi — LLM Engineering & AI Systems" />

<!-- Twitter / X -->
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Private RAG with LangChain, Chroma, and Local LLMs" />
<meta name="twitter:description" content="Hands-on RAG tutorial: private vector database with Chroma, LangChain retriever pipeline, and local LLMs for enterprise-ready AI search." />
<meta name="twitter:image" content="https://kikebodi.com/img/rag_diagram.png" />

  <!-- Bootstrap + Clean Blog -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet" />
  <link href="css/clean-blog.min.css" rel="stylesheet" />

  <!-- PrismJS (Okaidia dark theme) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-okaidia.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/plugins/line-numbers/prism-line-numbers.css" />

  <style>
    /* Force code blocks to look great and dark */
    pre[class*="language-"],
    code[class*="language-"] {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.95rem;
    }
    pre[class*="language-"] {
      border-radius: 10px;
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.35);
      overflow: auto;
      position: relative;
      padding-top: 3rem; /* space for copy button/header */
    }
    /* Code header (filename / label) */
    .code-header {
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 2.5rem;
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 0 0.75rem;
      color: #eaeaea;
      background: rgba(0, 0, 0, 0.5);
      border-top-left-radius: 10px;
      border-top-right-radius: 10px;
      font-size: 0.85rem;
      letter-spacing: 0.2px;
    }
    .copy-btn {
      border: 1px solid rgba(255, 255, 255, 0.25);
      background: rgba(255, 255, 255, 0.08);
      color: #fff;
      border-radius: 6px;
      padding: 0.25rem 0.5rem;
      cursor: pointer;
      transition: background 0.2s ease, border 0.2s ease;
    }
    .copy-btn:hover {
      background: rgba(255, 255, 255, 0.18);
      border-color: rgba(255, 255, 255, 0.45);
    }
    /* Improve table contrast on dark */
    @media (prefers-color-scheme: dark) {
      body {
        color: #eaeaea;
        background-color: #0f1115;
      }
      .navbar-light .navbar-brand,
      .navbar-light .navbar-nav .nav-link {
        color: #eaeaea !important;
      }
      .table {
        color: #eaeaea;
      }
      .table-bordered td,
      .table-bordered th {
        border-color: #2a2f3a;
      }
      .copyright {
        color: #9aa1aa !important;
      }
    }
    /* Masthead image tint for readability */
    .masthead .overlay {
      background-color: rgba(0, 0, 0, 0.45);
    }
  </style>
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "TechArticle",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kikebodi.com/private-rag-vector-database.html"
  },
  "headline": "Building a Private RAG System with LangChain, Chroma, and Local LLMs",
  "description": "A practical, end-to-end guide to building a private Retrieval-Augmented Generation (RAG) system using LangChain, Chroma, and open-source LLMs. Covers embeddings, chunking, vector search, pipelines, and productionization.",
  "keywords": [
    "RAG",
    "Retrieval-Augmented Generation",
    "Vector Databases",
    "Chroma",
    "LangChain",
    "Embeddings",
    "LLM Engineering",
    "AI Agents",
    "Ollama",
    "Private LLMs",
    "Enterprise RAG",
    "Token Optimization",
    "Document Indexing",
    "AI Workflows",
    "Multi-model pipelines"
  ],
  "author": {
    "@type": "Person",
    "name": "Enrique Bodí",
    "url": "https://kikebodi.com"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Enrique Bodí — LLM Engineering & AI Systems",
    "url": "https://kikebodi.com",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kikebodi.com/img/profile.jpg"
    }
  },
  "datePublished": "2025-11-23",
  "dateModified": "2025-11-23",
  "image": "https://kikebodi.com/img/rag_diagram.png",
  "articleSection": "LLM Engineering",
  "learningResourceType": "Tutorial",
  "proficiencyLevel": "Expert",
  "about": [
    {
      "@type": "Thing",
      "name": "LLM Engineering"
    },
    {
      "@type": "Thing",
      "name": "Private RAG Systems"
    },
    {
      "@type": "Thing",
      "name": "AI Infrastructure"
    },
    {
      "@type": "Thing",
      "name": "Enterprise AI Systems"
    }
  ]
}
</script>
</head>
<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="index.html">Kike Bodi</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="index.html">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="about.html">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="portfolio.html">Portfolio</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="contact.html">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="masthead" style="background-image: url('img/rag-vector-database-landscape.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <div class="post-heading">
          <h1>Building a Private RAG System with Ollama, LangChain and Chroma</h1>
          <h2 class="subheading">Part 1: Populate your private vector database</h2>
          <span class="meta">Posted by <a href="#">Kike Bodí</a> on November 2025</span>
        </div>
      </div>
    </div>
  </div>
</header>
  <article>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <p>
          This is the first part of a series building a privacy-first RAG system with open-source LLMs from
          HuggingFace (using Ollama), LangChain and Chroma.
        </p>

        <p>
          At the end of this blog series, we will have a fully functioning, production-ready RAG system for your
          company or organization.
        </p>

        <p>
          In this tutorial, everything will run locally as our main goal is to keep the data private within your
          organization. On the last section, we will suggest some improvements in case you are willing to sacrifice
          privacy for performance and scalability.
        </p>

        <h2 class="section-heading">Series Index</h2>
        <ol>
          <li><a href="#rag-prerequisites">Prerequisites</a></li>
          <li><a href="#rag-populate-db">Populate the Vector Database</a></li>
          <li><a href="./private-rag-retriever.html#rag-retriever">Vector Retriever</a></li>
          <li>RAG implementation</li>
          <li>Chat UI</li>
          <li>Evaluation</li>
          <li>Performance improvements</li>
        </ol>

        <h2 class="section-heading">How a RAG System Works (High Level)</h2>

        <img alt="" class="bi mb nh c" width="700" height="356" loading="eager" role="presentation" src="https://miro.medium.com/v2/resize:fit:1400/1*diTLYX2NBstoDrSjLglM5g.png">

        <div id="rag-prerequisites">
        <h2 class="section-heading">1. Prerequisites</h2>

        <p>
          We assume the knowledge base is already organized in folders and stored in Markdown (<code>.md</code>) files.
        </p>

        <p>
          If you’re just getting started or don’t yet have structured data, check the post on how to generate
          synthetic datasets (TODO) or get one from
          <a href="https://huggingface.co/datasets?modality=modality:document&amp;sort=trending"
             target="_blank" rel="noopener">
            HuggingFace
          </a>.
        </p>
        </div>

        <div id="rag-populate-db">
        <h2 class="section-heading">2. Populate the Vector Database</h2>

        <h3>What Is a Vector?</h3>

        <p>
          A vector is a multi-dimensional numerical representation of text. It encodes semantic meaning into
          coordinates inside a high-dimensional space. We’ll visualize these embeddings later.
        </p>

        <p>The pipeline is simple:</p>
        <ol>
          <li>Split documents into chunks.</li>
          <li>Generate embeddings for each chunk.</li>
          <li>Store them in a vector database.</li>
        </ol>

        <h3>2.1 Split the Knowledge Base into Chunks</h3>

        <p>Given an array of documents called <code>documents</code>:</p>

        <pre class="line-numbers"><code class="language-python">from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=3200, chunk_overlap=480)
chunks = text_splitter.split_documents(documents)</code></pre>

        <p>Two parameters matter:</p>
        <ul>
          <li><code>chunk_size</code>: number of characters per chunk.</li>
          <li><code>chunk_overlap</code>: overlap between consecutive chunks.</li>
        </ul>

        <p>
          A common standard is <strong>800 tokens</strong>, which equals roughly <strong>3200 characters</strong>
          (1 token ≈ 4 characters).
        </p>

        <p>
          We will optimise further on the evaluation section to find the sweet-spot between:
        </p>
        <ul>
          <li>Chunks too short → context fragmentation</li>
          <li>Chunks too long → retrieval noise</li>
        </ul>

        <p>Recommended overlap:</p>
        <ul>
          <li><strong>Light overlap</strong> (facts, code, structured text): 10–15%</li>
          <li><strong>Heavy overlap</strong> (legal, narrative, procedural): 20–30%</li>
        </ul>

        <p>For general company documentation, 15% works well:</p>
        <pre class="line-numbers"><code class="language-python">chunk_size = 3200
chunk_overlap = int(chunk_size * 0.15)  # 480</code></pre><br>

        <h3>2.2 Generate Embeddings</h3>

        <p>
          Now we create embeddings using an encoder model. These models differ from autoregressive LLMs—they do not
          generate text but instead convert text into dense vector representations.
        </p>

        <p>Example using HuggingFace:</p>
        <pre class="line-numbers"><code class="language-python">from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_openai import OpenAIEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# embeddings = OpenAIEmbeddings(model="text-embedding-3-large")</code></pre>

        <p>
          For commercial applications, OpenAI’s <code>text-embedding-3-small</code> and
          <code>text-embedding-3-large</code> give higher accuracy at a slightly higher cost.
        </p>

        <h3>2.3 Store the Embeddings in Chroma</h3>

        <pre id="code-populate-chroma" class="line-numbers"><code class="language-python">from langchain_community.vectorstores import Chroma
import os

DB_NAME = "company_rag_chroma"

# Reset collection if it already exists
if os.path.exists(DB_NAME):
    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=DB_NAME,
)</code></pre>

        <p>At this point, the vector database is populated.</p>

        <p>To inspect it:</p>

        <pre class="line-numbers"><code class="language-python">collection = vectorstore._collection
count = collection.count()

sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)

print(f"There are {count:,} vectors with {dimensions:,} dimensions in the vector store")</code></pre>

        <p>Example output:</p>

        <pre class="line-numbers"><code class="language-text">There are 413 vectors with 384 dimensions in the vector store</code></pre>

        <p>
          This means we have <strong>413 chunks</strong> represented in a <strong>384-dimensional space</strong>.
        </p>

        <h3>2.4 Visualizing Embeddings in 2D</h3>

        <p>
          Although impossible to understand high-dimensional space directly, we can reduce it using t-SNE.
        </p>

        <pre class="line-numbers"><code class="language-python">import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go

result = collection.get(include=["embeddings", "documents", "metadatas"])
vectors = np.array(result["embeddings"])
documents = result["documents"]
metadatas = result["metadatas"]

# Example: color by document type
DOC_TYPES = ["products", "employees", "contracts", "company"]
COLORS = ["blue", "green", "red", "orange"]

doc_types = [metadata["doc_type"] for metadata in metadatas]
colors = [COLORS[DOC_TYPES.index(t)] for t in doc_types]</code></pre>

        <p>Reduce to 2D:</p>

        <pre class="line-numbers"><code class="language-python">tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)</code></pre>

        <p>Plot:</p>

        <pre class="line-numbers"><code class="language-python">fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode="markers",
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}&lt;br&gt;Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo="text",
)])

fig.update_layout(title="2D Chroma Vector Store Visualization")
fig.show()</code></pre>
        </br>
        <img alt="" float: left loading="eager" role="presentation" src="./img/2d-chroma-data-visualization.png">

        <p>
          We observe a strong relation between products and contracts. An a softer one between company and emproyees/products.
        </p>

        <h3>2.5 Visualizing Embeddings in 3D</h3>
        </br>

        <pre class="line-numbers"><code class="language-python">tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode="markers",
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}&lt;br&gt;Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo="text",
)])

fig.update_layout(
    title="3D Chroma Vector Store Visualization",
    scene=dict(xaxis_title="x", yaxis_title="y", zaxis_title="z"),
    width=900,
    height=700,
    margin=dict(r=10, b=10, l=10, t=40)
)

fig.show()</code></pre>

        <img alt="" float: left loading="eager" role="presentation" src="./img/3d-chroma-data-visualization.gif">

        <p>
          Good job! Only with this, we got roughly <strong>25% of our private RAG system</strong>.
        </p>

        <p>
          We are ready now to move to the next part: <a href="./private-rag-retriever.html">
        Part 2: Create the vector retriever
      </a> 
        </p>
      </div>
   <hr />
<div class="container my-4">
  <h3>More LLM Engineering articles</h3>
  <ul>
    <li>
      <a href="https://kikebodi.com/llm-engineering-token-optimization.html">
        LLM Engineering | Token optimization
      </a> – caching, thin system prompts, and cost-optimized production usage.
    </li>
    <li>
      <a href="https://kikebodi.com/llm-engineering-basic-usage.html">
        LLM Engineering | Running local LLMs and APIs
      </a> – Ollama, OpenAI, Anthropic, OpenRouter, LangChain, and LiteLLM.
    </li>
  </ul>
</div>
    </div>
    </div>
  </div>
</article>

  <!-- JS -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="js/clean-blog.min.js"></script>

  <!-- PrismJS core + languages + plugins -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/plugins/line-numbers/prism-line-numbers.min.js"></script>
</body>
</html>